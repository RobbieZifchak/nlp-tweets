{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import string\n",
    "import re\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.probability import FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.min_rows', 50)\n",
    "pd.options.display.max_colwidth = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in tweets queried containing 'anxiety'\n",
    "df = pd.read_csv('csv_files/anxiety_tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing unwanted columns for this analysis\n",
    "df.drop(columns = ['user', 'date', 'retweet','mention', 'hashtags', 'location'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>It's okay to not totally understand the specific struggles of queer Japanese / Japanese people... but you can be open to learning about it. Just b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10 things to know before returning to your hair salon Excitement, anxiety and a new normal at salons and barber shops as Long Island enters Phase ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Well that information would have been useful 2 months ago, and avoided wasted anxiety every single time a person came even slightly close to enter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>My anxiety hasn’t been the greatest lately.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i was mostly afraid because i have really bad anxiety and we've had misunderstandings in the past so i kept a lot to myself but recently it reache...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                    text\n",
       "0  It's okay to not totally understand the specific struggles of queer Japanese / Japanese people... but you can be open to learning about it. Just b...\n",
       "1  10 things to know before returning to your hair salon Excitement, anxiety and a new normal at salons and barber shops as Long Island enters Phase ...\n",
       "2  Well that information would have been useful 2 months ago, and avoided wasted anxiety every single time a person came even slightly close to enter...\n",
       "3                                                                                                            My anxiety hasn’t been the greatest lately.\n",
       "4  i was mostly afraid because i have really bad anxiety and we've had misunderstandings in the past so i kept a lot to myself but recently it reache..."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sanity check\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "\n",
    "#add punctuation char's to stopwords list\n",
    "stop_words += list(string.punctuation)\n",
    "stop_words += ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'anxiety', 'rt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    stopwords_removed = [token.lower() for token in tokens if token.lower() not in stop_words]\n",
    "    return stopwords_removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(process_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking words to see where preprocessing failed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15                                                                            [season, 13, reasons, stressing, tf, already, clay, really, think, batman, ’]\n",
       "16    [fear, love, covid, horror, life, coronavirus, mentalhealth, art, depression, faith, hope, motivation, fearless, scary, dark, quotes, courage, cre...\n",
       "17                                                                                                   [feel, like, 7pm, cheer, slowly, morphed, 7pm, scream]\n",
       "18    [twitter, started, original, issues, twitter, wrong, place, get, help, unless, 's, really, intent, was.twitter, n't, professional, help, fact, thi...\n",
       "19    [thread, beautiful, moments, protests, ’, attended, nyc, past, week, half, post, intended, glorify, protests, show, wish, spread, bit, positivity,...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'][15:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['painting', 'batman', 'procreate', 'pocket', 'art', 'color', 'paint', 'iphone', 'finger', 'mark', 'stroke', 'brush', 'manhattan', 'new', 'york', 'https', '//www.instagram.com/p/b5lxqgylvx0/', 'igshid=j68k86tqc9l']\n"
     ]
    }
   ],
   "source": [
    "print(df['text'][9973])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results\n",
    "* Tokens appear to have been split effectively with some exceptions. \n",
    "* Hashtags and mentions have been removed. Standalone punctuation has been removed. \n",
    "* Stop words are not present. However, URL's have been split. In order to prevent the extra noise from the URL's, I will remove them prior to the split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iteration 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('csv_files/anxiety_tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing unwanted columns for this analysis\n",
    "df.drop(columns = ['user', 'date', 'retweet','mention', 'hashtags', 'location'], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proposed Steps\n",
    "\n",
    "* Remove Urls\n",
    "* Remove stopwords\n",
    "* Lowercase text\n",
    "* Analyze text to see what was missed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>It's okay to not totally understand the specific struggles of queer Japanese / Japanese people... but you can be open to learning about it. Just b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10 things to know before returning to your hair salon Excitement, anxiety and a new normal at salons and barber shops as Long Island enters Phase ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                    text\n",
       "0  It's okay to not totally understand the specific struggles of queer Japanese / Japanese people... but you can be open to learning about it. Just b...\n",
       "1  10 things to know before returning to your hair salon Excitement, anxiety and a new normal at salons and barber shops as Long Island enters Phase ..."
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove urls from text first\n",
    "def remove_urls(text):\n",
    "    return re.sub(r'http\\S+','', text)\n",
    "\n",
    "df['text'] = df['text'].apply(remove_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "\n",
    "#add punctuation char's to stopwords list\n",
    "stop_words += list(string.punctuation)\n",
    "stop_words += ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'anxiety', 'rt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    stopwords_removed = [token.lower() for token in tokens if token.lower() not in stop_words]\n",
    "    return stopwords_removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] =  df['text'].apply(process_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['painting',\n",
       " 'batman',\n",
       " 'procreate',\n",
       " 'pocket',\n",
       " 'art',\n",
       " 'color',\n",
       " 'paint',\n",
       " 'iphone',\n",
       " 'finger',\n",
       " 'mark',\n",
       " 'stroke',\n",
       " 'brush',\n",
       " 'manhattan',\n",
       " 'new',\n",
       " 'york']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'][9973]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['season',\n",
       " '13',\n",
       " 'reasons',\n",
       " 'stressing',\n",
       " 'tf',\n",
       " 'already',\n",
       " 'clay',\n",
       " 'really',\n",
       " 'think',\n",
       " 'batman',\n",
       " '’']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'][15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "* While the url's have been dealt with, words like \"n't\", \"'s\", \"was.twitter\", \"7PM\", \"13, etc. all still remain.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = list(df['text'][15:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['feel', 'like', '7pm', 'cheer', 'slowly', 'morphed', '7pm', 'scream']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['feel', 'like', 'cheer', 'slowly', 'morphed', 'scream']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(filter(lambda x: x.isalpha(), test[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_nums(text_object):\n",
    "    no_nums = list(filter(lambda x: x.isalpha(), text_object))\n",
    "    return no_nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(remove_nums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [okay, totally, understand, specific, struggles, queer, japanese, japanese, people, open, learning, feel, personally, mean, exist, applies, racism...\n",
       "1                                      [things, know, returning, hair, salon, excitement, new, normal, salons, barber, shops, long, island, enters, phase]\n",
       "2    [well, information, would, useful, months, ago, avoided, wasted, every, single, time, person, came, even, slightly, close, entering, general, vici...\n",
       "3                                                                                                                                       [greatest, lately]\n",
       "4    [mostly, afraid, really, bad, misunderstandings, past, kept, lot, recently, reached, point, desperation, something, surprisingly, attentive, amp, ...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(df_text):\n",
    "    lemmatized =[]\n",
    "    for w in df_text:\n",
    "        lemmatized.append(lemmatizer.lemmatize(w))\n",
    "    return lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "testers = list(df['text'][15:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_tester = list(df['text'][9973])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['painting', 'batman', 'procreate', 'pocket', 'art', 'color', 'paint', 'iphone', 'finger', 'mark', 'stroke', 'brush', 'manhattan', 'new', 'york']\n"
     ]
    }
   ],
   "source": [
    "print(other_tester)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['season', 'reason', 'stressing', 'tf', 'already', 'clay', 'really', 'think', 'batman'], ['fear', 'love', 'covid', 'horror', 'life', 'coronavirus', 'mentalhealth', 'art', 'depression', 'faith', 'hope', 'motivation', 'fearless', 'scary', 'dark', 'quote', 'courage', 'creepy', 'corona', 'peace', 'poetry', 'success', 'stress', 'selflove', 'inspiration', 'terror', 'halloween', 'pain', 'formylus'], ['feel', 'like', 'cheer', 'slowly', 'morphed', 'scream'], ['twitter', 'started', 'original', 'issue', 'twitter', 'wrong', 'place', 'get', 'help', 'unless', 'really', 'intent', 'professional', 'help', 'fact', 'thing', 'said', 'often', 'make', 'situation', 'worse'], ['thread', 'beautiful', 'moment', 'protest', 'attended', 'nyc', 'past', 'week', 'half', 'post', 'intended', 'glorify', 'protest', 'show', 'wish', 'spread', 'bit', 'positivity', 'time', 'many', 'feeling', 'intense', 'anger', 'heartbreak']]\n"
     ]
    }
   ],
   "source": [
    "print(testers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine Vocab and Frequency Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115932 words total, with a vocabulary size of 14505\n",
      "Max tweet length is 35\n"
     ]
    }
   ],
   "source": [
    "all_words = [word for tokens in df['text'] for word in tokens]\n",
    "tweet_lengths = [len(tokens) for tokens in df['text']]\n",
    "vocab = sorted(list(set(all_words)))\n",
    "\n",
    "print('{} words total, with a vocabulary size of {}'.format(len(all_words), len(vocab)))\n",
    "print('Max tweet length is {}'.format(max(tweet_lengths)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_words = [item for sublist in df['text'] for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq = FreqDist(flat_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('like', 1066),\n",
       " ('time', 888),\n",
       " ('get', 864),\n",
       " ('people', 785),\n",
       " ('day', 684),\n",
       " ('depression', 633),\n",
       " ('feel', 625),\n",
       " ('much', 619),\n",
       " ('give', 600),\n",
       " ('amp', 590),\n",
       " ('one', 531),\n",
       " ('help', 527),\n",
       " ('know', 515),\n",
       " ('attack', 507),\n",
       " ('really', 479),\n",
       " ('stress', 479),\n",
       " ('go', 466),\n",
       " ('going', 465),\n",
       " ('u', 456),\n",
       " ('today', 454),\n",
       " ('thing', 452),\n",
       " ('need', 452),\n",
       " ('social', 428),\n",
       " ('make', 419),\n",
       " ('right', 390),\n",
       " ('work', 370),\n",
       " ('think', 357),\n",
       " ('good', 355),\n",
       " ('way', 355),\n",
       " ('fear', 353)]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_freq.most_common(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocess steps as one function\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess(df_text):\n",
    "    tokens = word_tokenize(df_text)\n",
    "    stopwords_removed = [token.lower() for token in tokens if token.lower() not in stop_words and len(token) > 3]\n",
    "    \n",
    "    lemmatized =[]\n",
    "    \n",
    "    for w in stopwords_removed:\n",
    "        lemmatized.append(lemmatizer.lemmatize(w))\n",
    "        \n",
    "    processed = list(filter(lambda x: x.isalpha(), lemmatized))  \n",
    "        \n",
    "    return processed\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "* initial preprocessing steps are complete\n",
    "* it is likely that I will add more stop words, and iteratively perform more cleaning steps once the modeling begins. But this is a good start.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
